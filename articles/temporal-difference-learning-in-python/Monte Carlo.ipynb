{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Monte Carlo Method](https://harderchoices.com/2018/04/04/monte-carlo-method-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo method: \n",
    "\n",
    "- reinforcement learnikng method for estimating value\n",
    "- broad class of computational algorithms that rely on repeated random sampling to obtain numerical results\n",
    "- using \"randomness\" to solve our problem\n",
    "\n",
    "In this notebook, we will use \"FrozenLake\" game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On-policy first-visit MC control__:\n",
    "\n",
    "- On-policy algorithm: is improving the policy it's working on (opposed to off-policy)[https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning]\n",
    "\n",
    "- first-visit algorithm averages returns for visits following the first vists to given state (opposed to every-visit)\n",
    "\n",
    "- MC control or Monte carlo conteol. This type of algorithm first estimates values for states, then builds greedy (or near greedy) policy on them just to use the new policy to create another estimate. It loops forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from gym.spaces.tuple_space import Tuple\n",
    "from gym.envs.registration import register\n",
    "import random\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env) # 1. \n",
    "        \n",
    "    Q = create_state_action_dictionary(env, policy) # 2.\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in range(episodes): # 4.\n",
    "        G = 0 # 5.\n",
    "        episode = run_game(env=env, policy=policy, display=False) # 6.\n",
    "        for i in reversed(range(0, len(episode))): # 7.\n",
    "            s_t, a_t, r_t = episode[i] # 8. \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # 9.\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # 10.\n",
    "                if returns.get(state_action): # 11.\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # 12.\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # 13.\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # 15.\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.monitor_interval = 0\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=200\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200\n",
    ")\n",
    "\n",
    "fl_slippery = {\n",
    "    'small': 'FrozenLake-v0',\n",
    "    'big': 'FrozenLake8x8-v0'\n",
    "}\n",
    "\n",
    "fl_not_slippery = {\n",
    "    'small': 'FrozenLakeNotSlippery-v0',\n",
    "    'big': 'FrozenLakeNotSlippery8x8-v0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(slippery=False, big=False):\n",
    "    if slippery:\n",
    "        env = gym.make(fl_slippery['big'] if big else fl_slippery['small'])\n",
    "    else:\n",
    "        env = gym.make(fl_not_slippery['big'] if big else fl_not_slippery['small'])\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy\n",
    "\n",
    "\n",
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "        Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q    \n",
    "\n",
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "            \n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(0.1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        \n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break   \n",
    "        \n",
    "        state, reward, finished, info =  env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "        \n",
    "        episode.append(timestep)\n",
    "        \n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(0.05)\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = create_environment(slippery=True, big=False)\n",
    "_ = run_game(env, create_random_policy(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4x4 not slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_environment(slippery=False, big=False)\n",
    "policy = monte_carlo_e_soft(env, episodes=200)\n",
    "test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = run_game(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8x8 not slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_environment(slippery=False, big=True)\n",
    "policy = monte_carlo_e_soft(env, episodes=10000)\n",
    "test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = run_game(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iterator(env, n, t, epsilon=0.01):\n",
    "    random_policy = create_random_policy(env)\n",
    "    random_policy_score = test_policy(random_policy, env)\n",
    "    best_policy = (random_policy, random_policy_score)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(t)):\n",
    "        new_policy =  monte_carlo_e_soft(env, policy=best_policy[0], episodes=n, epsilon=epsilon)\n",
    "        new_policy_score = test_policy(new_policy, env)\n",
    "        if new_policy_score > best_policy[1]:\n",
    "            best_policy = (new_policy, new_policy_score)\n",
    "            \n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:49<00:00, 34.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_environment(slippery=True, big=False)\n",
    "policy, score = policy_iterator(env, 50, 10000, epsilon=0.01)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-5.1.0]",
   "language": "python",
   "name": "conda-env-anaconda3-5.1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
